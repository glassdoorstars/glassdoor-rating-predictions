{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5004ddeb-e5ce-4a52-ace9-4ffe0ee0183f",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b321ed32-b5a6-45ad-b341-80679e381954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Quiet all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845303b3-d479-4e24-80b5-0bd4ccaccd8d",
   "metadata": {},
   "source": [
    "**Get data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5e587f-504e-4b7f-a323-4de6c06e790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pprint import pprint\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "def get_news_articles():\n",
    "    url = \"https://inshorts.com/en/read\"\n",
    "    # set different categories to perse through\n",
    "    category = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "    articles = {}\n",
    "    df_setup = []\n",
    "    for cat in category:\n",
    "        # read the url\n",
    "        res = get(url + \"/\" + category[0])\n",
    "        print(res)\n",
    "\n",
    "        # create a beautiful soup object\n",
    "        soup_parser = BeautifulSoup(res.content, 'html.parser').body\n",
    "\n",
    "        soup = soup_parser.find_all(\"span\", itemprop=\"mainEntityOfPage\")\n",
    "        for i in range(len(soup)):\n",
    "            link = soup[i][\"itemid\"]\n",
    "\n",
    "            article = get(link)\n",
    "            article_soup = BeautifulSoup(article.content,\"html.parser\").body\n",
    "\n",
    "            article_title = article_soup.find('span', itemprop='headline').text\n",
    "            article_body = article_soup.find('div', itemprop='articleBody').text\n",
    "            # articles[f\"article {cat} {i}\"] = [article_title, cat ,link ,article_body]\n",
    "\n",
    "            article_instance = {\n",
    "                'title': article_title,\n",
    "                'content': article_body,\n",
    "                'category': cat,\n",
    "            }\n",
    "\n",
    "            df_setup.append(article_instance)\n",
    "    return pd.DataFrame(df_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96414d74-8bb0-4207-94bc-adfd5902b6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Burger King to face US lawsuit claiming its Wh...</td>\n",
       "      <td>A US judge has rejected Burger King's bid to d...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SoftBank to sell 1.17% stake in Zomato for ₹94...</td>\n",
       "      <td>SoftBank Vision Fund is likely to offload the ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India is a 30:30:30 story: Union Minister Piyu...</td>\n",
       "      <td>Union Minister Piyush Goyal said when he think...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sri Lanka to import 92.1 mn eggs from India to...</td>\n",
       "      <td>Sri Lanka will import 92.1 million eggs from I...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How much will LPG cylinder cost in major citie...</td>\n",
       "      <td>The government has slashed the price of domest...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Burger King to face US lawsuit claiming its Wh...   \n",
       "1  SoftBank to sell 1.17% stake in Zomato for ₹94...   \n",
       "2  India is a 30:30:30 story: Union Minister Piyu...   \n",
       "3  Sri Lanka to import 92.1 mn eggs from India to...   \n",
       "4  How much will LPG cylinder cost in major citie...   \n",
       "\n",
       "                                             content  category  \n",
       "0  A US judge has rejected Burger King's bid to d...  business  \n",
       "1  SoftBank Vision Fund is likely to offload the ...  business  \n",
       "2  Union Minister Piyush Goyal said when he think...  business  \n",
       "3  Sri Lanka will import 92.1 million eggs from I...  business  \n",
       "4  The government has slashed the price of domest...  business  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating = get_news_articles()\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c4765-5839-4510-be86-a873e2b6f5b0",
   "metadata": {},
   "source": [
    "**Clean strings**\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d01e12d-70ae-40f1-8c31-a8281d4f0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    \"\"\"\n",
    "    This function puts a string in lowercase, normalizes any unicode characters, removes anything that         \n",
    "    isn't an alphanumeric symbol or single quote.\n",
    "    \"\"\"\n",
    "    # Normalize unicode characters\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Remove unwanted characters and put string in lowercase\n",
    "    string = re.sub(r\"[^\\w0-9'\\s]\", '', string).lower()\n",
    "            \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030cd05b-8b19-4584-83dc-932ced100e2f",
   "metadata": {},
   "source": [
    "**Lemmatize**\n",
    "- Apply lemmatization to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f38d959-c07b-40b8-9573-666da964a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    \"\"\"\n",
    "    This function takes in a string, lemmatizes each word, and returns a lemmatized version of the orignal string\n",
    "    \"\"\"\n",
    "    # Build the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Run the lemmatizer on each word after splitting the input string, store results in the 'results' list\n",
    "    results = []\n",
    "    for word in string.split():\n",
    "        results.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    # Convert results back into a string\n",
    "    string = ' '.join(results)\n",
    "    \n",
    "    # Return the resulting string\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1776f85-9f31-4126-805d-a3dc6a6c1045",
   "metadata": {},
   "source": [
    "**Remove stop words**\n",
    "- Remove all stop words from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d556471-5ad1-4957-9536-6efd2fa5dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words=None, exclude_words=None):\n",
    "    \"\"\"\n",
    "    Takes in a string, with optional arguments for words to add to stock stopwords and words to ignore in the \n",
    "    stock list removes the stopwords, and returns a stopword free version of the original string\n",
    "    \"\"\"\n",
    "    # Get the list of stopwords from nltk\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Create a set of stopwords to exclude\n",
    "    excluded_stopwords = set(exclude_words) if exclude_words else set()\n",
    "    \n",
    "    # Include any extra words in the stopwords to exclude\n",
    "    stopwords_to_exclude = set(stopword_list) - excluded_stopwords\n",
    "    \n",
    "    # Add extra words to the stopwords set\n",
    "    stopwords_to_exclude |= set(extra_words) if extra_words else set()\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    words = string.split()\n",
    "    \n",
    "    # Filter out stopwords from the tokenized words\n",
    "    filtered_words = [word for word in words if word not in stopwords_to_exclude]\n",
    "    \n",
    "    # Convert back to string\n",
    "    string = ' '.join(filtered_words)\n",
    "    \n",
    "    # Return the resulting string\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f6589-9dc7-4519-b8f3-51b29ba626d4",
   "metadata": {},
   "source": [
    "**Split data**\n",
    "- Apply a 60% training, 20% Validation, and 20% testing split to the data. (Random state 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2bf6ea6-6c50-427f-bf00-fa2b3e39d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_readmes(df):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and performs a 70/15/15 split. Outputs a train, validate, and test dataframe\n",
    "    \"\"\"\n",
    "    # Perfrom a 70/15/15 split\n",
    "    train_val, test = train_test_split(df, test_size=.2, random_state=95)\n",
    "    train, validate = train_test_split(train_val, test_size=.25, random_state=95)\n",
    "    \n",
    "    # Return the dataframe slices\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede4b95-c6c8-4b37-93a7-f984f324c457",
   "metadata": {},
   "source": [
    "**Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a96995dd-b58f-4e3e-9568-d1017e8afe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_readmes(df, col:str=\"content_column\"):\n",
    "    \"\"\"\n",
    "    Takes in the dataframe and the column name that contains the corpus data, creates a column of cleaned data, then uses that \n",
    "    to create a column without stopwords that is lemmatized, performs a train-validate-test split, and returns train, validate,\n",
    "    and test.\n",
    "    \"\"\"\n",
    "    # Initialize a list to collect cleaned elements in the for-loop below\n",
    "    cleaned_row = []\n",
    "    \n",
    "    # Iterate through the readme_content values...\n",
    "    for i in df[col].values:\n",
    "        \n",
    "        # Clean each value in the column and append to the 'cleaned_row' list\n",
    "        cleaned_row.append(clean(i))\n",
    "        \n",
    "    # Assign the clean row content to a new column in the dataframe named 'cleaned_content\n",
    "    df = df.assign(cleaned_content=cleaned_row)\n",
    "    \n",
    "    # Using a lambda, lemmatize all values in the 'cleaned_content' column and assign to a new column called 'lemmatized'\n",
    "    df['lemmatized'] = df['cleaned_content'].apply(lambda x: lemmatize(remove_stopwords(x)))\n",
    "    \n",
    "    # Split the dataframe (70/15/15)\n",
    "    train, validate, test = split_readmes(df)\n",
    "    \n",
    "    # Return train, validate, and test dataframes\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "635e3d9a-c9b5-4343-8e8a-bf1cd09005dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = prep_readmes(rating, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a586efa-0866-4d7e-bc08-4eeb891e6ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 5), (8, 5), (8, 5))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5a9498c-f335-48c8-832c-231b1df46a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How much will LPG cylinder cost in major citie...</td>\n",
       "      <td>The government has slashed the price of domest...</td>\n",
       "      <td>business</td>\n",
       "      <td>the government has slashed the price of domest...</td>\n",
       "      <td>government slashed price domestic lpg cylinder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SoftBank to sell 1.17% stake in Zomato for ₹94...</td>\n",
       "      <td>SoftBank Vision Fund is likely to offload the ...</td>\n",
       "      <td>sports</td>\n",
       "      <td>softbank vision fund is likely to offload the ...</td>\n",
       "      <td>softbank vision fund likely offload 117 stake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sri Lanka to import 92.1 mn eggs from India to...</td>\n",
       "      <td>Sri Lanka will import 92.1 million eggs from I...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>sri lanka will import 921 million eggs from in...</td>\n",
       "      <td>sri lanka import 921 million egg india counter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sri Lanka to import 92.1 mn eggs from India to...</td>\n",
       "      <td>Sri Lanka will import 92.1 million eggs from I...</td>\n",
       "      <td>business</td>\n",
       "      <td>sri lanka will import 921 million eggs from in...</td>\n",
       "      <td>sri lanka import 921 million egg india counter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Influencers mis-sell saying they can make you ...</td>\n",
       "      <td>Zerodha CEO Nithin Kamath said influencers mis...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>zerodha ceo nithin kamath said influencers mis...</td>\n",
       "      <td>zerodha ceo nithin kamath said influencers mis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "4   How much will LPG cylinder cost in major citie...   \n",
       "11  SoftBank to sell 1.17% stake in Zomato for ₹94...   \n",
       "33  Sri Lanka to import 92.1 mn eggs from India to...   \n",
       "3   Sri Lanka to import 92.1 mn eggs from India to...   \n",
       "39  Influencers mis-sell saying they can make you ...   \n",
       "\n",
       "                                              content       category  \\\n",
       "4   The government has slashed the price of domest...       business   \n",
       "11  SoftBank Vision Fund is likely to offload the ...         sports   \n",
       "33  Sri Lanka will import 92.1 million eggs from I...  entertainment   \n",
       "3   Sri Lanka will import 92.1 million eggs from I...       business   \n",
       "39  Zerodha CEO Nithin Kamath said influencers mis...  entertainment   \n",
       "\n",
       "                                      cleaned_content  \\\n",
       "4   the government has slashed the price of domest...   \n",
       "11  softbank vision fund is likely to offload the ...   \n",
       "33  sri lanka will import 921 million eggs from in...   \n",
       "3   sri lanka will import 921 million eggs from in...   \n",
       "39  zerodha ceo nithin kamath said influencers mis...   \n",
       "\n",
       "                                           lemmatized  \n",
       "4   government slashed price domestic lpg cylinder...  \n",
       "11  softbank vision fund likely offload 117 stake ...  \n",
       "33  sri lanka import 921 million egg india counter...  \n",
       "3   sri lanka import 921 million egg india counter...  \n",
       "39  zerodha ceo nithin kamath said influencers mis...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf17da-fd55-4a42-9b8c-6d0f0da93715",
   "metadata": {},
   "source": [
    "## Preparation actions taken\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "- Lemmatize\n",
    "- Remove stop words\n",
    "- 60, 20, 20 split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
